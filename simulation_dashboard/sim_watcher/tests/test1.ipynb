{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JobID           Partition               State   Elapsed     User\n",
      "0   22242853  regular_milan_ss11             TIMEOUT  00:30:01  joeschm\n",
      "1   22243206  regular_milan_ss11             TIMEOUT  00:30:04  joeschm\n",
      "2   22243472  regular_milan_ss11             TIMEOUT  00:30:05  joeschm\n",
      "3   22243871  regular_milan_ss11           COMPLETED  00:23:18  joeschm\n",
      "4   22246009  regular_milan_ss11             TIMEOUT  00:30:26  joeschm\n",
      "5   22246122  regular_milan_ss11           COMPLETED  00:12:40  joeschm\n",
      "6   22246517  regular_milan_ss11             TIMEOUT  00:30:18  joeschm\n",
      "7   22255027  regular_milan_ss11             TIMEOUT  00:30:14  joeschm\n",
      "8   22255214  regular_milan_ss11             TIMEOUT  00:30:15  joeschm\n",
      "9   22255293  regular_milan_ss11           COMPLETED  00:25:24  joeschm\n",
      "10  22257129  regular_milan_ss11             TIMEOUT  00:30:22  joeschm\n",
      "11  22257165  regular_milan_ss11           COMPLETED  00:00:21  joeschm\n",
      "12  22261717  regular_milan_ss11           COMPLETED  00:16:12  joeschm\n",
      "13  22261741  regular_milan_ss11           COMPLETED  00:23:43  joeschm\n",
      "14  22261844  regular_milan_ss11           COMPLETED  00:08:57  joeschm\n",
      "15  22262261  regular_milan_ss11           COMPLETED  00:08:29  joeschm\n",
      "16  22262638  regular_milan_ss11  CANCELLED by 95065  00:02:25  joeschm\n",
      "17  22262764  regular_milan_ss11           COMPLETED  00:21:53  joeschm\n",
      "18  22263592  regular_milan_ss11           COMPLETED  00:08:22  joeschm\n",
      "19  22264139  regular_milan_ss11           COMPLETED  00:07:58  joeschm\n",
      "20  22264933  regular_milan_ss11             TIMEOUT  00:30:25  joeschm\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "def run_command(cmd):\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if error:\n",
    "        raise Exception(f\"Error executing command '{cmd}': {error.decode('utf-8')}\")\n",
    "\n",
    "    return output.decode('utf-8')\n",
    "\n",
    "def get_slurm_jobs(username=None):\n",
    "    base_cmd = \"squeue -o '%.18i %.9P %.8j %.8u %.2t %.10M %.6D %R'\"\n",
    "    if username:\n",
    "        base_cmd += f\" -u {username}\"\n",
    "    output = run_command(base_cmd)\n",
    "\n",
    "    lines = output.split('\\n')[1:]  # Skip the header line\n",
    "    data = [line.split() for line in lines if line]  # Skip empty lines\n",
    "    return data\n",
    "\n",
    "def get_slurm_accounting_data(username=None):\n",
    "    base_cmd = \"sacct -X --format=JobID,Partition,State,Elapsed,User --parsable2\"\n",
    "    if username:\n",
    "        base_cmd += f\" -u {username}\"\n",
    "    output = run_command(base_cmd)\n",
    "\n",
    "    lines = output.split('\\n')[2:]  # Skip the header lines\n",
    "    data = [line.split('|') for line in lines if line]  # Skip empty lines\n",
    "    return data\n",
    "\n",
    "def convert_to_dataframe(data, columns):\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "username = 'joeschm'  # Specify the username here\n",
    "# jobs_data = get_slurm_jobs(username)\n",
    "# jobs_df = convert_to_dataframe(jobs_data, ['JobID', 'Partition', 'Name', 'User', 'State', 'Time', 'Nodes', 'Nodelist'])\n",
    "# print(jobs_df)\n",
    "\n",
    "# For accounting data\n",
    "acct_data = get_slurm_accounting_data(username)\n",
    "acct_df = convert_to_dataframe(acct_data, ['JobID', 'Partition', 'State', 'Elapsed', 'User'])\n",
    "print(acct_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       JobID               Submit                Start                  End  \\\n",
      "0   22111430  2024-02-23T16:17:10  2024-02-23T16:23:27  2024-02-23T16:53:54   \n",
      "1   22113855  2024-02-23T18:04:44  2024-02-23T18:05:12  2024-02-23T18:24:58   \n",
      "2   22114095  2024-02-23T18:11:11  2024-02-23T18:11:18  2024-02-23T18:41:26   \n",
      "3   22114264  2024-02-23T18:27:12  2024-02-23T18:28:42  2024-02-23T18:33:49   \n",
      "4   22136583  2024-02-24T16:09:38  2024-02-24T16:09:48  2024-02-24T16:40:17   \n",
      "5   22138909  2024-02-24T17:33:45  2024-02-24T17:39:15  2024-02-24T17:45:42   \n",
      "6   22175721  2024-02-25T14:58:27  2024-02-25T14:59:14  2024-02-25T15:04:34   \n",
      "7   22175776  2024-02-25T15:00:33  2024-02-25T15:20:49  2024-02-25T15:25:57   \n",
      "8   22176043  2024-02-25T15:24:51                 None  2024-02-25T15:44:08   \n",
      "9   22176362  2024-02-25T15:30:27  2024-02-25T15:53:39  2024-02-25T16:38:15   \n",
      "10  22176591  2024-02-25T15:45:38  2024-02-25T15:45:52  2024-02-25T15:46:31   \n",
      "11  22176686  2024-02-25T15:47:16  2024-02-25T15:47:25  2024-02-25T16:17:52   \n",
      "12  22177052  2024-02-25T16:10:48  2024-02-25T16:15:55  2024-02-25T16:40:44   \n",
      "13  22177481  2024-02-25T16:32:25                 None  2024-02-25T16:33:14   \n",
      "14  22177484  2024-02-25T16:34:20  2024-02-25T16:38:37  2024-02-25T17:08:37   \n",
      "15  22177506  2024-02-25T16:36:03  2024-02-25T16:40:46  2024-02-25T17:05:53   \n",
      "16  22177928  2024-02-25T17:03:18  2024-02-25T17:06:05  2024-02-25T17:30:57   \n",
      "17  22177936  2024-02-25T17:04:49  2024-02-25T17:08:48  2024-02-25T17:39:07   \n",
      "18  22178139  2024-02-25T17:16:38  2024-02-25T17:30:58  2024-02-25T17:52:00   \n",
      "19  22180757  2024-02-25T18:48:56  2024-02-25T18:51:39  2024-02-25T20:29:51   \n",
      "20  22180852  2024-02-25T18:51:53  2024-02-25T18:52:52  2024-02-25T19:22:53   \n",
      "21  22181829  2024-02-25T19:23:58  2024-02-25T19:31:02  2024-02-25T20:01:16   \n",
      "22  22205556  2024-02-26T09:36:47  2024-02-26T09:36:56  2024-02-26T10:45:01   \n",
      "23  22205661  2024-02-26T09:40:23  2024-02-26T09:45:40  2024-02-26T09:46:03   \n",
      "24  22205820  2024-02-26T09:44:06  2024-02-26T09:46:37  2024-02-26T09:47:02   \n",
      "25  22206191  2024-02-26T09:51:33  2024-02-26T13:49:03  2024-02-26T15:03:21   \n",
      "26  22206210  2024-02-26T09:52:15  2024-02-26T14:21:18  2024-02-26T14:37:14   \n",
      "27  22206345  2024-02-26T09:55:44  2024-02-26T18:27:32  2024-02-26T19:37:13   \n",
      "28  22226694  2024-02-26T19:50:22  2024-02-26T19:50:47  2024-02-26T20:00:27   \n",
      "29  22226710  2024-02-26T19:53:39  2024-02-26T19:54:43  2024-02-26T19:58:13   \n",
      "30  22226720  2024-02-26T19:54:32                 None  2024-02-26T19:55:28   \n",
      "31  22226735  2024-02-26T19:55:48  2024-02-26T19:58:40  2024-02-26T20:07:32   \n",
      "32  22227429  2024-02-26T20:05:12  2024-02-26T20:05:36  2024-02-26T20:35:51   \n",
      "33  22227627  2024-02-26T20:10:16  2024-02-26T20:10:40  2024-02-26T20:40:54   \n",
      "34  22227738  2024-02-26T20:15:31  2024-02-26T20:36:15  2024-02-26T21:06:32   \n",
      "35  22227751  2024-02-26T20:17:27  2024-02-26T20:41:17  2024-02-26T21:11:32   \n",
      "\n",
      "   NNodes        QOS               State   Elapsed     User  \n",
      "0       6      debug             TIMEOUT  00:30:27  joeschm  \n",
      "1       6      debug           COMPLETED  00:19:46  joeschm  \n",
      "2       6      debug             TIMEOUT  00:30:08  joeschm  \n",
      "3       6      debug           COMPLETED  00:05:07  joeschm  \n",
      "4       6      debug             TIMEOUT  00:30:29  joeschm  \n",
      "5       6      debug           COMPLETED  00:06:27  joeschm  \n",
      "6       6      debug           COMPLETED  00:05:20  joeschm  \n",
      "7       6  regular_1           COMPLETED  00:05:08  joeschm  \n",
      "8       6  regular_1  CANCELLED by 95065  00:00:00  joeschm  \n",
      "9       6  regular_1           COMPLETED  00:44:36  joeschm  \n",
      "10      6      debug  CANCELLED by 95065  00:00:39  joeschm  \n",
      "11      6      debug             TIMEOUT  00:30:27  joeschm  \n",
      "12      6      debug           COMPLETED  00:24:49  joeschm  \n",
      "13      6      debug  CANCELLED by 95065  00:00:00  joeschm  \n",
      "14      6      debug             TIMEOUT  00:30:00  joeschm  \n",
      "15      6      debug           COMPLETED  00:25:07  joeschm  \n",
      "16      6      debug           COMPLETED  00:24:52  joeschm  \n",
      "17      6      debug             TIMEOUT  00:30:19  joeschm  \n",
      "18      6      debug           COMPLETED  00:21:02  joeschm  \n",
      "19      6  regular_1           COMPLETED  01:38:12  joeschm  \n",
      "20      6      debug             TIMEOUT  00:30:01  joeschm  \n",
      "21      6      debug             TIMEOUT  00:30:14  joeschm  \n",
      "22      6  regular_1           COMPLETED  01:08:05  joeschm  \n",
      "23      6  regular_1           COMPLETED  00:00:23  joeschm  \n",
      "24      6  regular_1           COMPLETED  00:00:25  joeschm  \n",
      "25     12  regular_1           COMPLETED  01:14:18  joeschm  \n",
      "26     12  regular_1           COMPLETED  00:15:56  joeschm  \n",
      "27     12  regular_1           COMPLETED  01:09:41  joeschm  \n",
      "28      6      debug           COMPLETED  00:09:40  joeschm  \n",
      "29      6      debug           COMPLETED  00:03:30  joeschm  \n",
      "30     12  regular_1  CANCELLED by 95065  00:00:00  joeschm  \n",
      "31      6      debug           COMPLETED  00:08:52  joeschm  \n",
      "32      6      debug             TIMEOUT  00:30:15  joeschm  \n",
      "33      6      debug             TIMEOUT  00:30:14  joeschm  \n",
      "34      6      debug             TIMEOUT  00:30:17  joeschm  \n",
      "35      6      debug             TIMEOUT  00:30:15  joeschm  \n",
      "DataFrame saved to slurm_jobs_data.csv\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "def run_command(cmd):\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    if error:\n",
    "        raise Exception(f\"Error executing command '{cmd}': {error.decode('utf-8')}\")\n",
    "\n",
    "    return output.decode('utf-8')\n",
    "\n",
    "def get_slurm_accounting_data_chunked(username=None, days_back=180, chunk_size=30):\n",
    "    all_data = []\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=chunk_size)\n",
    "\n",
    "    while days_back > 0:\n",
    "        start_time = start_date.strftime(\"%Y-%m-%d\")\n",
    "        end_time = end_date.strftime(\"%Y-%m-%d\")\n",
    "        # Update the format here to match the requested columns\n",
    "        format_str = \"JobID,Submit,Start,End,NNodes,QOS,State,Elapsed,User\"\n",
    "        base_cmd = f\"sacct -X --format={format_str} --parsable2 --starttime={start_time} --endtime={end_time}\"\n",
    "        if username:\n",
    "            base_cmd += f\" -u {username}\"\n",
    "        output = run_command(base_cmd)\n",
    "\n",
    "        lines = output.split('\\n')[2:]  # Skip the header lines\n",
    "        data = [line.split('|') for line in lines if line]  # Skip empty lines\n",
    "        all_data.extend(data)\n",
    "\n",
    "        # Update dates for the next chunk\n",
    "        end_date = start_date\n",
    "        start_date -= timedelta(days=chunk_size)\n",
    "        days_back -= chunk_size\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def convert_to_dataframe(data, columns):\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def load_existing_data(filename):\n",
    "#     try:\n",
    "#         existing_df = pd.read_csv(filename)\n",
    "#         if not existing_df.empty:\n",
    "#             # Assuming 'Submit' is in YYYY-MM-DD format; adjust parsing as necessary\n",
    "#             latest_date = pd.to_datetime(existing_df['Submit']).max()\n",
    "#             return existing_df, latest_date\n",
    "#     except FileNotFoundError:\n",
    "#         pass\n",
    "#     return pd.DataFrame(), None\n",
    "\n",
    "\n",
    "\n",
    "# def adjusted_days_back(latest_date):\n",
    "#     if latest_date is not None:\n",
    "#         now = datetime.now()\n",
    "#         delta = now - latest_date\n",
    "#         return max(0, delta.days - 1)  # Subtract 1 to avoid overlap; adjust as needed\n",
    "#     return 180  # Default days_back value if no existing data\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "username = 'joeschm'  # Specify the username here\n",
    "days_back = 10  # Specify the total number of days back you want to fetch data for\n",
    "chunk_size = 10  # Specify the chunk size for each query\n",
    "acct_data = get_slurm_accounting_data_chunked(username, days_back, chunk_size)\n",
    "acct_df = convert_to_dataframe(acct_data, ['JobID', 'Submit', 'Start', 'End', 'NNodes', 'QOS', 'State', 'Elapsed', 'User'])\n",
    "print(acct_df)\n",
    "\n",
    "\n",
    "filename = 'slurm_jobs_data.csv'  # Specify your desired path and filename\n",
    "acct_df.to_csv(filename, index=False)\n",
    "print(f\"DataFrame saved to {filename}\")\n",
    "\n",
    "\n",
    "# # Load existing data and determine how far back to query\n",
    "# filename = 'slurm_jobs_data.csv'\n",
    "# existing_df, latest_date = load_existing_data(filename)\n",
    "# new_days_back = adjusted_days_back(latest_date)\n",
    "\n",
    "\n",
    "\n",
    "# # Fetch new data based on adjusted days_back\n",
    "# if new_days_back > 0:\n",
    "#     acct_data = get_slurm_accounting_data_chunked(username, new_days_back, chunk_size)\n",
    "#     new_df = convert_to_dataframe(acct_data, ['JobID', 'Submit', 'Start', 'End', 'NNodes', 'QOS', 'State', 'Elapsed', 'User'])\n",
    "\n",
    "#     # Combine new data with existing data\n",
    "#     combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "# else:\n",
    "#     combined_df = existing_df  # No new data to fetch\n",
    "\n",
    "# # Save combined DataFrame to CSV\n",
    "# combined_df.to_csv(filename, index=False)\n",
    "# print(f\"Updated DataFrame saved to {filename}\")\n",
    "\n",
    "# print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
